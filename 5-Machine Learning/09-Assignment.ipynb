{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment-09\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature engineering is the process of selecting, manipulating, and transforming raw data into features that can be used in supervised learning.\n",
    "- In order to make machine learning work well on new tasks, it might be necessary to design and train better features.\n",
    "- Feature engineering in ML consists of four main steps: Feature Creation, Transformations, Feature Extraction, and Feature Selection.\n",
    "- Feature engineering consists of creation, transformation, extraction, and selection of features, also known as variables, that are most conducive to creating an accurate ML algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in.\n",
    "- Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.\n",
    "- There are three types of feature selection:\n",
    "  - **Wrapper methods** (forward, backward, and stepwise selection)\n",
    "  - **Filter methods** (ANOVA, Pearson correlation, variance thresholding)\n",
    "  - **Embedded methods** (Lasso, Ridge, Decision Tree).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main differences between the filter and wrapper methods for feature selection are:\n",
    "- Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "- The filter method has the fastest running time; however, it does not consider feature dependencies and tends to each feature separately when univariate techniques are used.\n",
    "- The wrapper method has the advantages of better generalization and robust interaction with the classifier used for feature selection and is computationally expensive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Answer the following\n",
    "\n",
    "- Describe the overall feature selection process.\n",
    "- Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature selection is the process of reducing the number of input variables when developing a predictive model where selected input contributes to significant amount in prediction accuracy.\n",
    "- It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n",
    "- Feature extraction refers to the process of transforming raw data into numerical features that can be processed while preserving the information in the original data set.\n",
    "- It aims to reduce the number of features in a dataset by creating new features from the existing ones (and then discarding the original features).\n",
    "- Algorithm:\n",
    "  - Principal Component Analysis\n",
    "  - Linear Discriminant Analysis\n",
    "  - Independent Component Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Describe the feature engineering process in the sense of a text categorization issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text classification is the problem of assigning categories to text data according to its content.\n",
    "- The most important part of text classification is feature engineering: the process of creating features for a machine learning model from raw text data.\n",
    "- Text feature engineering commonly includes:\n",
    "  - Sanitize the text of punctuation, emojis, social media abbreviation, etc.\n",
    "  - We need to stemming or lemmatization to extract the root word.\n",
    "  - Need to create bag of words (n-grams)\n",
    "  - Convert text to number/vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cosine similarity is a metric used to measure how similar the documents are irrespective of their size.\n",
    "- The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.\n",
    "- Cosine similarity is the cosine of the angle between two n-dimensional vectors in an n-dimensional space.\n",
    "- It is the dot product of the two vectors divided by the product of the two vectors' lengths (or magnitudes).\n",
    "- Cosine Similarity: 0.675\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6753032524419089"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "a = [2, 3, 2, 0, 2, 3, 3, 0, 1]\n",
    "b = [2, 1, 0, 0, 3, 2, 1, 3, 1]\n",
    "1 - cosine(a, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Answer the following\n",
    "\n",
    "- What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "- Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Hamming distance between two vectors is the number of bits we must change to change one into the other.\n",
    "- Example Find the distance between the vectors `01101010` and `11011011`.\n",
    "- They differ in four places, so the Hamming distance `d(01101010,11011011) = 4`.\n",
    "- Hamming gap `d(10001011, 11001111) = 2`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25, 2.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import hamming\n",
    "\n",
    "a = [1, 0, 0, 0, 1, 0, 1, 1]\n",
    "b = [1, 1, 0, 0, 1, 1, 1, 1]\n",
    "hamming(a, b), len(a) * 0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666667, 0.5, 0.2857142857142857)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import jaccard\n",
    "\n",
    "a = [1, 1, 0, 0, 1, 0, 1, 1]\n",
    "b = [1, 1, 0, 0, 0, 1, 1, 1]\n",
    "c = [1, 0, 0, 1, 1, 0, 0, 1]\n",
    "\n",
    "1 - jaccard(a, b), 1 - jaccard(a, c), 1 - jaccard(b, c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. State what is meant by \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High-dimensional data are defined as data in which the number of features (variables observed), $p$, are close to or larger than the number of observations (or data points), $n$.\n",
    "- For example, tomographic imaging data, ECG data, and MEG data, microarrays, which measure gene expression, can contain tens of hundreds of samples.\n",
    "- When the dimensionality increases, the volume of the space increases so fast that the available data become sparse.\n",
    "- In order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Make a few quick notes on:\n",
    "\n",
    "- PCA is an acronym for Principal Computer Analysis.\n",
    "- Use of vectors\n",
    "- Embedded technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Principal component analysis (PCA) is a technique used for identification of a smaller number of uncorrelated variables known as principal components from a larger set of data.\n",
    "- The technique is widely used to emphasize variation and capture strong patterns in a data set.\n",
    "- It is also used to reduce dimensionality of data depending upon the no. of principal component which captures relatively large variation in the data.\n",
    "\n",
    "---\n",
    "\n",
    "- Vector are one dimensional data which is a tuple of one or more values of scalars.\n",
    "- In machine learning, feature vectors are used to represent numeric or symbolic characteristics, called features, of an object in a mathematical, easily analyzable way.\n",
    "- Most commonly in physics, vectors are used to represent displacement, velocity, and acceleration. Vectors are a combination of magnitude and direction, and are drawn as arrows.\n",
    "\n",
    "---\n",
    "\n",
    "- In the context of machine learning, an embedding is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors.\n",
    "- Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Make a comparison between:\n",
    "\n",
    "- Sequential backward exclusion VS sequential forward selection\n",
    "- Function selection methods: filter VS wrapper\n",
    "- SMC VS Jaccard coefficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sequential floating forward selection (SFFS) starts from the empty set.\n",
    "- After each forward step, SFFS performs backward steps as long as the objective function increases.\n",
    "- SFFS will select the feature which has higher contribution.\n",
    "- Sequential floating backward selection (SFBS) starts from the full set.\n",
    "- After each step, SFBS will drop the feature if it does not contribute much or above specified threshold.\n",
    "\n",
    "---\n",
    "\n",
    "- Filter methods pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate statistics instead of cross-validation performance.\n",
    "- Computationally less expensive compared to wrapper methods.\n",
    "- Eg.: information gain, statistical tests, correlation coefficient, etc.\n",
    "- Wrapper methods measure the “usefulness” of features based on the classifier performance.\n",
    "- Computationally more expensive compared to filter methods.\n",
    "- Eg.: recursive feature elimination, sequential feature selection, etc.\n",
    "\n",
    "---\n",
    "\n",
    "- The simple matching coefficient (SMC) or Rand similarity coefficient is a statistic used for **comparing** the similarity and diversity of sample sets.\n",
    "- SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe\n",
    "- The Jaccard index, also known as the Jaccard similarity coefficient, is a statistic used for **gauging** the similarity and diversity of sample sets.\n",
    "- Jaccard index only counts mutual presence as matches and compares it to the number of attributes that have been chosen by at least one of the two sets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('fsds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20f5f825ba7e1e0138bea706cf7ce76ba178632e6be331b5251920b13e58ac94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
