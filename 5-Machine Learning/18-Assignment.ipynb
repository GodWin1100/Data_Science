{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Assignment-18\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main distinction between the two approaches is the use of labeled datasets.\n",
    "- To put it simply, supervised learning uses labeled input and output data, while an unsupervised learning algorithm does not.\n",
    "- Unsupervised learning models, in contrast, work on their own to discover the inherent structure of unlabeled data.\n",
    "- In Supervised learning, you train the machine using data which is well “labeled.”\n",
    "- Unsupervised learning is a machine learning technique, where you do not need to supervise the model.\n",
    "- For example, Regression learning requires labelled target data where as clustering algorithm does not require labelled target as they do clustering based on the independent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mention a few unsupervised learning applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main applications of unsupervised learning includes:-\n",
    "  - Clustering\n",
    "  - Visualization\n",
    "  - Dimensionality reduction\n",
    "  - Finding association rules\n",
    "  - Anomaly detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What are the three main types of clustering methods? Briefly describe the characteristics of each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The various types of clustering are:\n",
    "- **Connectivity-based Clustering (Hierarchical clustering):**\n",
    "  - Hierarchical Clustering is a method of unsupervised machine learning clustering where it begins with a pre-defined top to bottom hierarchy of clusters.\n",
    "  - It then proceeds to perform a decomposition of the data objects based on this hierarchy, hence obtaining the clusters.\n",
    "  - This method follows two approaches based on the direction of progress, i.e., whether it is the top-down or bottom-up flow of creating clusters.\n",
    "- **Centroids-based Clustering (Partitioning methods):**\n",
    "  - Centroid based clustering is considered as one of the most simplest clustering algorithms, yet the most effective way of creating clusters and assigning data points to it.\n",
    "  - The intuition behind centroid based clustering is that a cluster is characterized and represented by a central vector and data points that are in close proximity to these vectors are assigned to the respective clusters.\n",
    "- **Density-based Clustering (Model-based methods):**\n",
    "  - If one looks into the previous two methods that we discussed, one would observe that both hierarchical and centroid based algorithms are dependent on a distance (similarity/proximity) metric.\n",
    "  - The very definition of a cluster is based on this metric.\n",
    "  - Density-based clustering methods take density into consideration instead of distances.\n",
    "  - Clusters are considered as the densest region in a data space, which is separated by regions of lower object density and it is defined as a maximal-set of connected points.\n",
    "- **Distribution-based Clustering:**\n",
    "  - Until now, the clustering techniques as we know are based around either proximity (similarity/distance) or composition (density).\n",
    "  - There is a family of clustering algorithms that take a totally different metric into consideration – probability.\n",
    "  - Distribution-based clustering creates and groups data points based on their likely hood of belonging to the same probability distribution (Gaussian, Binomial etc.) in the data.\n",
    "- **Fuzzy Clustering:**\n",
    "  - The general idea about clustering revolves around assigning data points to mutually exclusive clusters, meaning, a data point always resides uniquely inside a cluster and it cannot belong to more than one cluster.\n",
    "  - Fuzzy clustering methods change this paradigm by assigning a data-point to multiple clusters with a quantified degree of belongingness metric.\n",
    "  - The data-points that are in proximity to the center of a cluster, may also belong in the cluster that is at a higher degree than points in the edge of a cluster.\n",
    "  - The possibility of which an element belongs to a given cluster is measured by membership coefficient that vary from 0 to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explain how the `k-means` algorithm determines the consistency of clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **The Elbow Method**\n",
    "  - Calculate the Within-Cluster-Sum of Squared (WCSS) for different values of $k$, and choose the k for which WSS becomes first starts to diminish.\n",
    "  - In the plot of WCSS-versus-k, this is visible as an elbow.\n",
    "- **The Silhouette Method**\n",
    "  - The silhouette value measures how similar a point is to its own cluster (cohesion) compared to other clusters (separation).\n",
    "  - The range of the Silhouette value is between +1 and -1.\n",
    "  - A high value is desirable and indicates that the point is placed in the correct cluster.\n",
    "  - If many points have a negative Silhouette value, it may indicate that we have created too many or too few clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. With a simple illustration, explain the key difference between the `k-means` and `k-medoids` algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `k-means` attempts to minimize the total squared error, while `k-medoids` minimizes the sum of dissimilarities between points labeled to be in a cluster and a point designated as the center of that cluster.\n",
    "- `k-means` algorithm computes the center which is not a data point it's an virtual original centre of the cluster, hence it's not interpretable.\n",
    "- In contrast to the `k-means` algorithm, `k-medoids` chooses datapoints as centers, which are actual interpretable data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What is a dendrogram, and how does it work? Explain how to do it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A dendrogram is a diagram that shows the hierarchical relationship between objects.\n",
    "- It is most commonly created as an output from hierarchical clustering.\n",
    "- The dendrogram consists of stacked branches (called clades) that break down into further smaller branches.\n",
    "- At the lowest level will be individual elements and then they are grouped according to attributes into clusters with fewer and fewer clusters on higher levels.\n",
    "- The end of each clade (called a leaf) is the data.\n",
    "- The arrangement of the clades reveals how similar they are to each other; two leaves in the same clade are more similar than two leaves in another clade.\n",
    "- The y-axis (the height of the branch) shows how close data points or clusters are from one another.\n",
    "- The taller the branch, the further and more different the clusters are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What exactly is `SSE`? What role does it play in the `k-means` algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `SSE` is defined as the sum of the squared distance between centroid and each member of the cluster.\n",
    "- Since this is a measure of error, the objective of `k-means` is to try to minimize this value.\n",
    "- We plot a $K$ against `SSE` graph.\n",
    "- And from graph it's observed that as $K$ increases SSE decreases as distortion will be small.\n",
    "- It is used in Elbow Method to determine optimal value of $K$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. With a step-by-step algorithm, explain the `k-means` procedure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume $k$ clusters) fixed apriori.\n",
    "- The main idea is to define $k$ centers, one for each cluster.\n",
    "  1. Choose the number of clusters $k$.\n",
    "  2. Select $k$ random points from the data as centroids.\n",
    "  3. Assign all the points to the closest cluster centroid.\n",
    "  4. Recompute the centroids of newly formed clusters.\n",
    "  5. Repeat steps `3` and `4`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. In the sense of hierarchical clustering, define the terms single link and complete link.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Single link** You link two clusters based on the minimum distance between 2 elements.  \n",
    "  ![Single Linkage](./a18-9-1.png)\n",
    "- **Complete link** You link two clusters based on the max distance between 2 elements.  \n",
    "  ![Complete Linkage](./a18-9-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. How does the `apriori` concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Apriori` algorithm assumes that any subset of a frequent itemset must be frequent.\n",
    "- Its the algorithm behind Market Basket Analysis.\n",
    "- So, according to the principle of `Apriori`, if `{Grapes, Apple, Mango}` is frequent, then `{Grapes,Mango}` must also be frequent.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e4a015a5e8b3d416719d76fc156fe7e163a5f8678adc2f1ead049a4ae2a6091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
